{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handwritten Neural Network test: HandwrittenNN.ipynb\n",
    "# Author: comtalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def sigmoid_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 8)\n",
    "    '''\n",
    "    A = sigmoid(Z)\n",
    "    return np.multiply(A,(1 - A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.max(0, Z)\n",
    "    return A\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 9)\n",
    "    '''\n",
    "    if Z < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(Z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(Z)\n",
    "    else\n",
    "        return Z\n",
    "\n",
    "def activate_deriv(Z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid_deriv(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu_deriv(Z)\n",
    "    else\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: input in [features x samples]\n",
    "# W: learnable parameters in [layer x n(l) x n(l-1)]\n",
    "# b: bias parameters in [layer x n(l) x 1]\n",
    "# activations: an arraylist of string, size of l, denotes preferred activation for each layer\n",
    "#   example: {relu, relu, sidmoid} means relu in l = 1,2, sigmoid in l = 3\n",
    "def forward_propagation(X, W, b, activations):\n",
    "    L = W.shape[0]          # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    # initialize linear output\n",
    "    '''\n",
    "    This np.ndarray allow us to contruct an array with size initialized and can have any data type in it (from dtype=object)\n",
    "    Therefore, we could use this to create histogram-like array for the uneven NN\n",
    "    '''\n",
    "    Z = np.ndarray(shape=[L+1], dtype=object)\n",
    "    A = np.ndarray(shape=[L+1], dtype=object)\n",
    "    \n",
    "    # base case\n",
    "    Z[1] = np.dot(W[1], X) + b[1]\n",
    "    # activation\n",
    "    if activations[0] == \"sigmoid\":\n",
    "        A[1] = sigmoid(Z[1])\n",
    "    elif activations[0] == \"relu\":\n",
    "        A[1] = relu(Z[1])\n",
    "    else:\n",
    "        A[1] = Z[1]\n",
    "\n",
    "    # loop the layers\n",
    "    for l in range(2, L):\n",
    "        Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "        if activations[l-1] == \"sigmoid\":\n",
    "            A[l] = sigmoid(Z[l])\n",
    "        elif activations[l-1] == \"relu\":\n",
    "            A[l] = relu(Z[l])\n",
    "        else:\n",
    "            A[l] = Z[l]\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Z, A, W, b, Y, activations):\n",
    "    L = W.shape[0]          # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    dZ = np.ndarray(shape=Z.shape, dtype=object)\n",
    "    dA = np.ndarray(shape=A.shape, dtype=object)\n",
    "    dW = np.ndarray(shape=W.shape, dtype=object)\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 10)\n",
    "    '''\n",
    "    #A[0] = X                # to make it work properly when l = 1 \n",
    "\n",
    "    # base case\n",
    "    print(type(A[L]))\n",
    "    print(type(Y))\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = (1/m)*np.dot(dZ[L], A[L-1].T)\n",
    "    db[L] = (1/m)*np.sum(dZ[L], axis = 1, keepdims = True) \n",
    "\n",
    "    # loop the layers\n",
    "    for l in range(L-1, 1):\n",
    "        dA[l] = np.dot(W[l+1].T, dZ[l+1])\n",
    "        dZ[l] = np.multiply(dA[l], activate_deriv(Z[l], activations[l]) )\n",
    "        dW[l] = (1/m)*np.dot(dZ[l], A[l-1].T)\n",
    "        db[l] = (1/m)*np.sum(dZ[l], axis = 1, keepdims = True) \n",
    "    \n",
    "    return dA, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########      BELOW THIS LINE IS EXPERIMENTAL AREA, THE CODE MAY BE MESSY      ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.23042070e-307, 2.22523004e-307, 1.69120145e-306,\n        9.34583987e-307],\n       [4.67296746e-307, 1.29062229e-306, 9.34598246e-307,\n        1.24610723e-306],\n       [1.69121096e-306, 1.69121367e-306, 9.34599604e-307,\n        2.04722549e-306],\n       [1.29061821e-306, 8.45603440e-307, 7.56593696e-307,\n        4.45051101e-307],\n       [2.22522053e-306, 5.11799242e-307, 1.33511562e-306,\n        6.23060065e-307]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb = np.ndarray(shape=[4, 5])\n",
    "dumb = dumb.T\n",
    "dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.rand(n, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    return A, Z\n",
    "\n",
    "def backward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.randn(n, m)*10) % 10\n",
    "    Y = np.round(np.random.randn(n2, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    print(type(A[L]))\n",
    "    print(type(Y))\n",
    "    dA, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "    print(\"dA\")\n",
    "    print(dA)\n",
    "    print(\"db\")\n",
    "    print(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X\n[[5. 8. 6. 3. 4.]\n [2. 6. 9. 3. 2.]]\nW\n[None array([[3., 0.],\n       [4., 2.],\n       [1., 8.]])\n array([[7., 6., 0.],\n       [7., 9., 4.],\n       [1., 0., 5.],\n       [1., 1., 9.]])]\nb\n[None array([[2.],\n       [3.],\n       [6.]])\n array([[8.],\n       [5.],\n       [8.],\n       [7.]])]\nA\n[None\n array([[17., 26., 20., 11., 14.],\n       [27., 47., 45., 21., 23.],\n       [27., 62., 84., 33., 26.]])\n array([[289., 472., 418., 211., 244.],\n       [475., 858., 886., 403., 414.],\n       [160., 344., 448., 184., 152.],\n       [294., 638., 828., 336., 278.]])\n None]\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'NoneType'>\n<class 'numpy.ndarray'>\n"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-0214b258693a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbackward_propagation_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-c25cb736a972>\u001b[0m in \u001b[0;36mbackward_propagation_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-d3ae210e448b>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[1;34m(X, Z, A, W, b, Y, activations)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "backward_propagation_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37664bit961b0c540318489d9163391a342af877",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}