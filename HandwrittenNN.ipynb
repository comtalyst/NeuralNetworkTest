{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handwritten Neural Network test: HandwrittenNN.ipynb\n",
    "# Author: comtalyst\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "- \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def sigmoid_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 8)\n",
    "    '''\n",
    "    A = sigmoid(Z)\n",
    "    return np.multiply(A,(1 - A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.max(0, Z)\n",
    "    return A\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 9)\n",
    "    '''\n",
    "    if Z < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(Z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(Z)\n",
    "    else:\n",
    "        return Z\n",
    "\n",
    "def activate_deriv(Z, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid_deriv(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu_deriv(Z)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are untested, will be tested when running descent\n",
    "def compute_loss(Y_, Y):\n",
    "    loss = -(np.dot(Y, np.log(Y_).T) + np.dot(1-Y, np.log(1-Y_).T) )\n",
    "    return loss\n",
    "\n",
    "def compute_cost(Y_, Y):\n",
    "    cost = np.mean(compute_loss(Y_, Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: input in [features x samples]\n",
    "# W: learnable parameters in [layer x n(l) x n(l-1)]\n",
    "# b: bias parameters in [layer x n(l) x 1]\n",
    "# activations: an arraylist of string, size of l, denotes preferred activation for each layer\n",
    "#   example: {relu, relu, sidmoid} means relu in l = 1,2, sigmoid in l = 3\n",
    "def forward_propagation(X, W, b, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    # initialize linear output\n",
    "    '''\n",
    "    This np.ndarray allow us to contruct an array with size initialized and can have any data type in it (from dtype=object)\n",
    "    Therefore, we could use this to create histogram-like array for the uneven NN\n",
    "    '''\n",
    "    Z = np.ndarray(shape=[L+1], dtype=object)\n",
    "    A = np.ndarray(shape=[L+1], dtype=object)\n",
    "    \n",
    "    # base case\n",
    "    Z[1] = np.dot(W[1], X) + b[1]\n",
    "    # activation\n",
    "    A[1] = activate(Z[1], activations[0])\n",
    "\n",
    "    # loop the layers 2 to L\n",
    "    for l in range(2, L+1):\n",
    "        Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "        A[l] = activate(Z[l], activations[l-1])     # note that activations[]'s index is behind for the ease of user\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is untested, will be test using gradient checking\n",
    "def backward_propagation(X, Z, A, W, b, Y, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    dZ = np.ndarray(shape=Z.shape, dtype=object)\n",
    "    dA = np.ndarray(shape=A.shape, dtype=object)\n",
    "    dW = np.ndarray(shape=W.shape, dtype=object)\n",
    "    db = np.ndarray(shape=b.shape, dtype=object)\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 10)\n",
    "    '''\n",
    "    A[0] = X                # to make it work properly when l = 1 \n",
    "\n",
    "    # base case\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = (1/m)*np.dot(dZ[L], A[L-1].T)\n",
    "    db[L] = (1/m)*np.sum(dZ[L], axis = 1, keepdims = True) \n",
    "\n",
    "    # loop the layers L-1 to 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA[l] = np.dot(W[l+1].T, dZ[l+1])\n",
    "        dZ[l] = np.multiply(dA[l], activate_deriv(Z[l], activations[l]) )\n",
    "        dW[l] = (1/m)*np.dot(dZ[l], A[l-1].T)\n",
    "        db[l] = (1/m)*np.sum(dZ[l], axis = 1, keepdims = True) \n",
    "    \n",
    "    return dA, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########      BELOW THIS LINE IS EXPERIMENTAL AREA, THE CODE MAY BE MESSY      ##########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.23042070e-307, 2.22523004e-307, 1.69120145e-306,\n        9.34583987e-307],\n       [4.67296746e-307, 1.29062229e-306, 9.34598246e-307,\n        1.24610723e-306],\n       [1.69121096e-306, 1.69121367e-306, 9.34599604e-307,\n        2.04722549e-306],\n       [1.29061821e-306, 8.45603440e-307, 7.56593696e-307,\n        4.45051101e-307],\n       [2.22522053e-306, 5.11799242e-307, 1.33511562e-306,\n        6.23060065e-307]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb = np.ndarray(shape=[4, 5])\n",
    "dumb = dumb.T\n",
    "dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.rand(n, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    return A, Z\n",
    "\n",
    "def backward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.randn(n, m)*10) % 10\n",
    "    Y = np.round(np.random.randn(n2, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    dA, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "    print(\"dA\")\n",
    "    print(dA)\n",
    "    print(\"db\")\n",
    "    print(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X\n[[9. 2. 3. 7. 7.]\n [9. 1. 1. 2. 8.]]\nW\n[None array([[1., 3.],\n       [3., 5.],\n       [1., 4.]])\n array([[3., 6., 2.],\n       [0., 2., 3.],\n       [2., 5., 8.],\n       [5., 7., 9.]])]\nb\n[None array([[4.],\n       [6.],\n       [8.]])\n array([[1.],\n       [1.],\n       [1.],\n       [6.]])]\nA\n[None\n array([[40.,  9., 10., 17., 35.],\n       [78., 17., 20., 37., 67.],\n       [53., 14., 15., 23., 47.]])\n array([[ 695.,  158.,  181.,  320.,  602.],\n       [ 316.,   77.,   86.,  144.,  276.],\n       [ 895.,  216.,  241.,  404.,  782.],\n       [1229.,  296.,  331.,  557., 1073.]])]\ndA\n[None\n array([[ 9950.,  2357.,  2622.,  4506.,  8672.],\n       [17755.,  4186.,  4655.,  8042., 15455.],\n       [20402.,  4879.,  5403.,  9197., 17805.]])\n None]\ndb\n[None array([[ 5621.4],\n       [10018.6],\n       [11537.2]])\n array([[386.2],\n       [175.2],\n       [501.4],\n       [692. ]])]\n"
    }
   ],
   "source": [
    "backward_propagation_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37664bit961b0c540318489d9163391a342af877",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}