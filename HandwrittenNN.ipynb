{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nTODO:\\n- continue on fundamentals structure (probably start with initialize variables)\\n- test backprop by gradient checking \\n- test cost function by running grad\\n- generate test data if possible\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handwritten Neural Network test: HandwrittenNN.ipynb\n",
    "# Author: comtalyst\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "- continue on fundamentals structure (probably start with initialize variables)\n",
    "- test backprop by gradient checking \n",
    "- test cost function by running grad\n",
    "- generate test data if possible\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation functions\n",
    "\n",
    "## Sigmoid\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def sigmoid_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 8)\n",
    "    '''\n",
    "    A = sigmoid(Z)\n",
    "    return np.multiply(A,(1 - A))\n",
    "\n",
    "## ReLu\n",
    "def relu(Z):\n",
    "    A = np.max(0, Z)\n",
    "    return A\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 9)\n",
    "    '''\n",
    "    if Z < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "## Global interface\n",
    "def activate(Z, activation):\n",
    "    activation = lower(activation)\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(Z)\n",
    "    else:\n",
    "        return Z\n",
    "\n",
    "def activate_deriv(Z, activation):\n",
    "    activation = lower(activation)\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid_deriv(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu_deriv(Z)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cost functions\n",
    "\n",
    "# These are untested, will be tested when running descent\n",
    "def compute_loss(Y_, Y):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 4)\n",
    "    (C1W4 , Block 5)\n",
    "    '''\n",
    "    loss = -(np.dot(Y, np.log(Y_).T) + np.dot(1-Y, np.log(1-Y_).T) )\n",
    "    return loss\n",
    "\n",
    "def compute_cost(Y_, Y):\n",
    "    cost = np.mean(compute_loss(Y_, Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization\n",
    "\n",
    "def initialize_parameters(layers):\n",
    "    L = len(layers)-1\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "    for l in range(1, L+1):\n",
    "        W[l] = np.random.randn(layers[l], layers[l-1])\n",
    "        b[l] = np.zeros((layers[l], 1))\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "\n",
    "# X: input in [features x samples]\n",
    "# W: learnable parameters in [layer x n(l) x n(l-1)]\n",
    "# b: bias parameters in [layer x n(l) x 1]\n",
    "# activations: an arraylist of string, size of l, denotes preferred activation for each layer\n",
    "#   example: {relu, relu, sidmoid} means relu in l = 1,2, sigmoid in l = 3\n",
    "def forward_propagation(X, W, b, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    # initialize linear output\n",
    "    '''\n",
    "    This np.ndarray allow us to contruct an array with size initialized and can have any data type in it (from dtype=object)\n",
    "    Therefore, we could use this to create histogram-like array for the uneven NN\n",
    "    '''\n",
    "    Z = np.ndarray(shape=[L+1], dtype=object)\n",
    "    A = np.ndarray(shape=[L+1], dtype=object)\n",
    "    \n",
    "    # base case\n",
    "    Z[1] = np.dot(W[1], X) + b[1]\n",
    "    # activation\n",
    "    A[1] = activate(Z[1], activations[0])\n",
    "\n",
    "    # loop the layers 2 to L\n",
    "    for l in range(2, L+1):\n",
    "        Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "        A[l] = activate(Z[l], activations[l-1])     # note that activations[]'s index is behind for the ease of user\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backward Propagation\n",
    "\n",
    "# This is untested, will be tested using gradient checking\n",
    "def backward_propagation(X, Z, A, W, b, Y, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    dZ = np.ndarray(shape=Z.shape, dtype=object)\n",
    "    dA = np.ndarray(shape=A.shape, dtype=object)\n",
    "    dW = np.ndarray(shape=W.shape, dtype=object)\n",
    "    db = np.ndarray(shape=b.shape, dtype=object)\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 10)\n",
    "    '''\n",
    "    A[0] = X                # to make it work properly when l = 1 \n",
    "\n",
    "    # base case\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = (1/m)*np.dot(dZ[L], A[L-1].T)\n",
    "    db[L] = (1/m)*np.sum(dZ[L], axis = 1, keepdims = True) \n",
    "\n",
    "    # loop the layers L-1 to 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA[l] = np.dot(W[l+1].T, dZ[l+1])\n",
    "        dZ[l] = np.multiply(dA[l], activate_deriv(Z[l], activations[l]) )\n",
    "        dW[l] = (1/m)*np.dot(dZ[l], A[l-1].T)\n",
    "        db[l] = (1/m)*np.sum(dZ[l], axis = 1, keepdims = True) \n",
    "    \n",
    "    return dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update paremeters\n",
    "\n",
    "def update_parameters(W, b, dW, db, learning_rate=0.01):\n",
    "    W -= learning_rate*dW\n",
    "    b -= learning_rate*db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model\n",
    "\n",
    "def train_model(X, Y, layers, activations, learning_rate, iterations):\n",
    "    W, b = initialize_parameters(layers)\n",
    "    for i in range(0, iterations):\n",
    "        A, Z = forward_propagation(X, W, b, activations)\n",
    "        dW, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "        W, b = update_parameters(W, b, dW, db, learning_rate)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########      BELOW THIS LINE IS PERFORMANCE AREA      ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process raw data\n",
    "## read file\n",
    "path = \"data/\"\n",
    "filename = \"2d_2color_dots.txt\"\n",
    "f = open(path + filename, \"r\")\n",
    "\n",
    "## generate a long, combined string from the file\n",
    "fstr = f.read()\n",
    "fstr = fstr.replace('\\n', ' ')\n",
    "fstr = fstr.strip()\n",
    "\n",
    "## process file\n",
    "flist = fstr.split(' ')\n",
    "fnparray = np.array(flist)\n",
    "m = len(flist)//3\n",
    "\n",
    "## ready to put to our inputs\n",
    "X = np.ndarray([2, m])\n",
    "Y_r = np.ndarray([1, m])                        # Y here is not ready for training because it is not a 0-1 (correct-incorrect) array yet\n",
    "X[0] = fnparray[0:m].astype(float)\n",
    "X[1] = fnparray[m:2*m].astype(float)\n",
    "Y_r = fnparray[2*m:].astype(int)\n",
    "\n",
    "## convert Y to 0-1 2d array\n",
    "# in this case, we have two possible outcomes (true-false)\n",
    "Y = np.ndarray([2, m])\n",
    "Y[0] = (Y_r == 0)\n",
    "Y[1] = (Y_r == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and train a model from provided data\n",
    "layers = [2, 3, 3, 3, 2]            # first and last is defined by input type (2D coords, true-false)\n",
    "activations = ['']\n",
    "W, b = train_model(X, Y, layers, activations, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##########      BELOW THIS LINE IS EXPERIMENTAL AREA, THE CODE MAY BE MESSY      ##########\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.23042070e-307, 2.22523004e-307, 1.69120145e-306,\n        9.34583987e-307],\n       [4.67296746e-307, 1.29062229e-306, 9.34598246e-307,\n        1.24610723e-306],\n       [1.69121096e-306, 1.69121367e-306, 9.34599604e-307,\n        2.04722549e-306],\n       [1.29061821e-306, 8.45603440e-307, 7.56593696e-307,\n        4.45051101e-307],\n       [2.22522053e-306, 5.11799242e-307, 1.33511562e-306,\n        6.23060065e-307]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb = np.ndarray(shape=[4, 5])\n",
    "dumb = dumb.T\n",
    "dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.rand(n, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    return A, Z\n",
    "\n",
    "def backward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.randn(n, m)*10) % 10\n",
    "    Y = np.round(np.random.randn(n2, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    dA, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "    print(\"dA\")\n",
    "    print(dA)\n",
    "    print(\"db\")\n",
    "    print(db)\n",
    "\n",
    "def initialize_parameters_test():\n",
    "    layers = [2, 3, 4]\n",
    "    m = 3\n",
    "\n",
    "    W, b = initialize_parameters(layers)\n",
    "    print(\"W: \" + str(W))\n",
    "    print(\"b: \" + str(b))\n",
    "\n",
    "    X = np.round(np.random.randn(layers[0], m)*10) % 10\n",
    "    print(\"X: \" + str(X))\n",
    "\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"A: \" + str(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "W: [None array([[4., 4.],\n       [0., 4.],\n       [5., 1.]])\n array([[4., 8., 6.],\n       [2., 0., 6.],\n       [4., 7., 5.],\n       [1., 5., 5.]])]\nb: [None array([[0.],\n       [0.],\n       [0.]])\n array([[0.],\n       [0.],\n       [0.],\n       [0.]])]\nX: [[2. 8. 2.]\n [6. 9. 6.]]\nA: [None\n array([[32., 68., 32.],\n       [24., 36., 24.],\n       [16., 49., 16.]])\n array([[416., 854., 416.],\n       [160., 430., 160.],\n       [376., 769., 376.],\n       [232., 493., 232.]])]\n"
    }
   ],
   "source": [
    "initialize_parameters_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37664bit961b0c540318489d9163391a342af877",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}