{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nTODO:\\n- continue on fundamentals structure (probably start with initialize variables)\\n- test backprop by gradient checking \\n- test cost function by running grad\\n- generate test data if possible\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handwritten Neural Network test: HandwrittenNN.ipynb\n",
    "# Author: comtalyst\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "- continue on fundamentals structure (probably start with initialize variables)\n",
    "- test backprop by gradient checking \n",
    "- test cost function by running grad\n",
    "- generate test data if possible\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation functions\n",
    "\n",
    "## Sigmoid\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def sigmoid_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 8)\n",
    "    '''\n",
    "    A = sigmoid(Z)\n",
    "    return np.multiply(A,(1 - A))\n",
    "\n",
    "## ReLu\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 9)\n",
    "    '''\n",
    "    A = ((Z >= np.zeros(Z.shape))).astype(int)\n",
    "    return A\n",
    "\n",
    "## Global interface\n",
    "def activate(Z, activation):\n",
    "    activation = activation.lower()\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu(Z)\n",
    "    else:\n",
    "        return Z\n",
    "\n",
    "def activate_deriv(Z, activation):\n",
    "    activation = activation.lower()\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid_deriv(Z)\n",
    "    elif activation == \"relu\":\n",
    "        return relu_deriv(Z)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cost functions\n",
    "\n",
    "# These are untested, will be tested when running descent\n",
    "def compute_loss(Y_, Y):\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 4)\n",
    "    (C1W4 , Block 5)\n",
    "    '''\n",
    "    loss = -(np.dot(Y, np.log(Y_).T) + np.dot(1-Y, np.log(1-Y_).T) )\n",
    "    return loss\n",
    "\n",
    "def compute_cost(Y_, Y):\n",
    "    '''\n",
    "    (C1W4, dnn_app_utils_v3.py)\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * compute_loss(Y_, Y)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization\n",
    "\n",
    "def initialize_parameters(layers):\n",
    "    L = len(layers)-1\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "    for l in range(1, L+1):\n",
    "        W[l] = np.random.randn(layers[l], layers[l-1])\n",
    "        b[l] = np.zeros((layers[l], 1))\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "\n",
    "# X: input in [features x samples]\n",
    "# W: learnable parameters in [layer x n(l) x n(l-1)]\n",
    "# b: bias parameters in [layer x n(l) x 1]\n",
    "# activations: an arraylist of string, size of l, denotes preferred activation for each layer\n",
    "#   example: {relu, relu, sidmoid} means relu in l = 1,2, sigmoid in l = 3\n",
    "def forward_propagation(X, W, b, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    # initialize linear output\n",
    "    '''\n",
    "    This np.ndarray allow us to contruct an array with size initialized and can have any data type in it (from dtype=object)\n",
    "    Therefore, we could use this to create histogram-like array for the uneven NN\n",
    "    '''\n",
    "    Z = np.ndarray(shape=[L+1], dtype=object)\n",
    "    A = np.ndarray(shape=[L+1], dtype=object)\n",
    "    \n",
    "    # base case\n",
    "    Z[1] = np.dot(W[1], X) + b[1]\n",
    "    # activation\n",
    "    A[1] = activate(Z[1], activations[0])\n",
    "\n",
    "    # loop the layers 2 to L\n",
    "    for l in range(2, L+1):\n",
    "        Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "        A[l] = activate(Z[l], activations[l-1])     # note that activations[]'s index is behind for the ease of user\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backward Propagation\n",
    "\n",
    "# This is untested, will be tested using gradient checking\n",
    "def backward_propagation(X, Z, A, W, b, Y, activations):\n",
    "    L = W.shape[0]-1        # layers\n",
    "    n = X.shape[0]          # features\n",
    "    m = X.shape[1]          # samples\n",
    "\n",
    "    dZ = np.ndarray(shape=Z.shape, dtype=object)\n",
    "    dA = np.ndarray(shape=A.shape, dtype=object)\n",
    "    dW = np.ndarray(shape=W.shape, dtype=object)\n",
    "    db = np.ndarray(shape=b.shape, dtype=object)\n",
    "    '''\n",
    "    (Coursera DL Notes, p. 10)\n",
    "    '''\n",
    "    A[0] = X                # to make it work properly when l = 1 \n",
    "\n",
    "    # base case\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = (1/m)*np.dot(dZ[L], A[L-1].T)\n",
    "    db[L] = (1/m)*np.sum(dZ[L], axis = 1, keepdims = True) \n",
    "\n",
    "    # loop the layers L-1 to 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        dA[l] = np.dot(W[l+1].T, dZ[l+1])\n",
    "        dZ[l] = np.multiply(dA[l], activate_deriv(Z[l], activations[l]) )\n",
    "        dW[l] = (1/m)*np.dot(dZ[l], A[l-1].T)\n",
    "        db[l] = (1/m)*np.sum(dZ[l], axis = 1, keepdims = True) \n",
    "    \n",
    "    return dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update paremeters\n",
    "\n",
    "def update_parameters(W, b, dW, db, learning_rate=0.01):\n",
    "    L = len(W)-1\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= np.multiply(learning_rate, dW[i])\n",
    "        b[i] -= np.multiply(learning_rate, db[i])\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model\n",
    "\n",
    "def train_model(X, Y, layers, activations, learning_rate, iterations):\n",
    "    W, b = initialize_parameters(layers)\n",
    "    for i in range(0, iterations):\n",
    "        A, Z = forward_propagation(X, W, b, activations)\n",
    "        if i%(iterations//100) == 0:\n",
    "            print(\"Iteration \" + str(i) + \": cost = \" + str(compute_cost(A[len(A)-1], Y)))\n",
    "        dW, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "        W, b = update_parameters(W, b, dW, db, learning_rate)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########      BELOW THIS LINE IS PERFORMANCE AREA      ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nNO NEED BECAUSE NOT MULTICLASS CLASSIFICATION\\n## convert Y to 0-1 2d array\\n# in this case, we have two possible outcomes (true-false)\\nY = np.ndarray([2, m])\\nY[0] = (Y_r == 0)\\nY[1] = (Y_r == 1)\\n'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Process raw data\n",
    "## read file\n",
    "path = \"data/\"\n",
    "filename = \"2d_2color_dots.txt\"\n",
    "f = open(path + filename, \"r\")\n",
    "\n",
    "## generate a long, combined string from the file\n",
    "fstr = f.read()\n",
    "fstr = fstr.replace('\\n', ' ')\n",
    "fstr = fstr.strip()\n",
    "\n",
    "## process file\n",
    "flist = fstr.split(' ')\n",
    "fnparray = np.array(flist)\n",
    "m = len(flist)//3\n",
    "\n",
    "## ready to put to our inputs\n",
    "X = np.ndarray([2, m])\n",
    "Y = np.ndarray([1, m])              \n",
    "X[0] = fnparray[0:m].astype(float)\n",
    "X[1] = fnparray[m:2*m].astype(float)\n",
    "Y[0] = fnparray[2*m:].astype(int)               # use [0] to prevent array conversion error\n",
    "\n",
    "'''\n",
    "NO NEED BECAUSE NOT MULTICLASS CLASSIFICATION\n",
    "## convert Y to 0-1 2d array\n",
    "# in this case, we have two possible outcomes (true-false)\n",
    "Y = np.ndarray([2, m])\n",
    "Y[0] = (Y_r == 0)\n",
    "Y[1] = (Y_r == 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 0: cost = 0.6931471805599454\nIteration 100: cost = 0.6918765996403385\nIteration 200: cost = 0.6911103671764353\nIteration 300: cost = 0.6906481347014851\nIteration 400: cost = 0.6903691845533172\nIteration 500: cost = 0.6902007775829846\nIteration 600: cost = 0.6900990720258189\nIteration 700: cost = 0.6900376308527434\nIteration 800: cost = 0.6900005044239061\nIteration 900: cost = 0.6899780658061235\nIteration 1000: cost = 0.6899645020117959\nIteration 1100: cost = 0.6899563018185777\nIteration 1200: cost = 0.6899513437454328\nIteration 1300: cost = 0.689948345700619\nIteration 1400: cost = 0.6899465327250764\nIteration 1500: cost = 0.689945436327007\nIteration 1600: cost = 0.6899447732526925\nIteration 1700: cost = 0.6899443722290873\nIteration 1800: cost = 0.6899441296862563\nIteration 1900: cost = 0.689943982991241\nIteration 2000: cost = 0.6899438942656653\nIteration 2100: cost = 0.6899438406011255\nIteration 2200: cost = 0.6899438081425114\nIteration 2300: cost = 0.6899437885100101\nIteration 2400: cost = 0.6899437766352812\nIteration 2500: cost = 0.689943769452814\nIteration 2600: cost = 0.6899437651084609\nIteration 2700: cost = 0.68994376248075\nIteration 2800: cost = 0.6899437608913574\nIteration 2900: cost = 0.6899437599299995\nIteration 3000: cost = 0.6899437593485127\nIteration 3100: cost = 0.6899437589967945\nIteration 3200: cost = 0.6899437587840538\nIteration 3300: cost = 0.6899437586553752\nIteration 3400: cost = 0.6899437585775428\nIteration 3500: cost = 0.6899437585304647\nIteration 3600: cost = 0.689943758501989\nIteration 3700: cost = 0.689943758484765\nIteration 3800: cost = 0.6899437584743471\nIteration 3900: cost = 0.6899437584680457\nIteration 4000: cost = 0.689943758464234\nIteration 4100: cost = 0.6899437584619286\nIteration 4200: cost = 0.6899437584605341\nIteration 4300: cost = 0.6899437584596909\nIteration 4400: cost = 0.6899437584591803\nIteration 4500: cost = 0.6899437584588718\nIteration 4600: cost = 0.689943758458685\nIteration 4700: cost = 0.6899437584585726\nIteration 4800: cost = 0.6899437584585042\nIteration 4900: cost = 0.6899437584584627\nIteration 5000: cost = 0.6899437584584376\nIteration 5100: cost = 0.6899437584584226\nIteration 5200: cost = 0.6899437584584135\nIteration 5300: cost = 0.6899437584584077\nIteration 5400: cost = 0.6899437584584046\nIteration 5500: cost = 0.6899437584584027\nIteration 5600: cost = 0.6899437584584014\nIteration 5700: cost = 0.6899437584584005\nIteration 5800: cost = 0.6899437584584004\nIteration 5900: cost = 0.6899437584584001\nIteration 6000: cost = 0.6899437584583998\nIteration 6100: cost = 0.6899437584583995\nIteration 6200: cost = 0.6899437584583998\nIteration 6300: cost = 0.6899437584583994\nIteration 6400: cost = 0.6899437584583995\nIteration 6500: cost = 0.6899437584583995\nIteration 6600: cost = 0.6899437584583997\nIteration 6700: cost = 0.6899437584583995\nIteration 6800: cost = 0.6899437584583994\nIteration 6900: cost = 0.6899437584583995\nIteration 7000: cost = 0.6899437584583995\nIteration 7100: cost = 0.6899437584583995\nIteration 7200: cost = 0.6899437584583995\nIteration 7300: cost = 0.6899437584583994\nIteration 7400: cost = 0.6899437584583993\nIteration 7500: cost = 0.6899437584583993\nIteration 7600: cost = 0.6899437584583995\nIteration 7700: cost = 0.6899437584583995\nIteration 7800: cost = 0.6899437584583994\nIteration 7900: cost = 0.6899437584583994\nIteration 8000: cost = 0.6899437584583993\nIteration 8100: cost = 0.6899437584583995\nIteration 8200: cost = 0.6899437584583994\nIteration 8300: cost = 0.6899437584583995\nIteration 8400: cost = 0.6899437584583994\nIteration 8500: cost = 0.6899437584583995\nIteration 8600: cost = 0.6899437584583995\nIteration 8700: cost = 0.6899437584583997\nIteration 8800: cost = 0.6899437584583995\nIteration 8900: cost = 0.6899437584583995\nIteration 9000: cost = 0.6899437584583995\nIteration 9100: cost = 0.6899437584583997\nIteration 9200: cost = 0.6899437584583995\nIteration 9300: cost = 0.6899437584583993\nIteration 9400: cost = 0.6899437584583995\nIteration 9500: cost = 0.6899437584583995\nIteration 9600: cost = 0.6899437584583995\nIteration 9700: cost = 0.6899437584583993\nIteration 9800: cost = 0.6899437584583994\nIteration 9900: cost = 0.6899437584583995\n"
    }
   ],
   "source": [
    "### Create and train a model from provided data\n",
    "\n",
    "## create a model\n",
    "np.random.seed(136)\n",
    "layers = [2, 3, 3, 3, 1]                                # first and last is defined by input type (2D coords, true-false)\n",
    "activations = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "learning_rate = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "W, b = train_model(X, Y, layers, activations, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##########      BELOW THIS LINE IS EXPERIMENTAL AREA, THE CODE MAY BE MESSY      ##########\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb = np.ndarray(shape=[4, 5])\n",
    "dumb = dumb.T\n",
    "dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.rand(n, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    return A, Z\n",
    "\n",
    "def backward_propagation_test():\n",
    "    L = 2\n",
    "    n1 = 3\n",
    "    n2 = 4\n",
    "    n = 2\n",
    "    m = 5\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "\n",
    "    X = np.round(np.random.randn(n, m)*10) % 10\n",
    "    Y = np.round(np.random.randn(n2, m)*10) % 10\n",
    "\n",
    "    W = np.ndarray(shape=[L+1], dtype=object)\n",
    "    b = np.ndarray(shape=[L+1], dtype=object)\n",
    "\n",
    "    W[1] = np.round(np.random.randn(n1, n)*10) % 10\n",
    "    W[2] = np.round(np.random.randn(n2, n1)*10) % 10\n",
    "    b[1] = np.round(np.random.randn(n1, 1)*10) % 10\n",
    "    b[2] = np.round(np.random.randn(n2, 1)*10) % 10\n",
    "\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"X\")\n",
    "    print(X)\n",
    "    print(\"W\")\n",
    "    print(W)\n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "    print(\"A\")\n",
    "    print(A)\n",
    "    dA, db = backward_propagation(X, Z, A, W, b, Y, activations)\n",
    "    print(\"dA\")\n",
    "    print(dA)\n",
    "    print(\"db\")\n",
    "    print(db)\n",
    "\n",
    "def initialize_parameters_test():\n",
    "    layers = [2, 3, 4]\n",
    "    m = 3\n",
    "\n",
    "    W, b = initialize_parameters(layers)\n",
    "    print(\"W: \" + str(W))\n",
    "    print(\"b: \" + str(b))\n",
    "\n",
    "    X = np.round(np.random.randn(layers[0], m)*10) % 10\n",
    "    print(\"X: \" + str(X))\n",
    "\n",
    "    activations = [\"dumb\", \"dumb\", \"dumb\"]\n",
    "    A, Z = forward_propagation(X, W, b, activations)\n",
    "    print(\"A: \" + str(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_parameters_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0.])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37664bit961b0c540318489d9163391a342af877",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}